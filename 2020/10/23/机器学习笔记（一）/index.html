<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习笔记（一） | AのBlog</title><meta name="description" content="学习资源网易：吴恩达《机器学习》 B站：[中英字幕]吴恩达机器学习系列课程 其他资料：Coursera-ML-AndrewNg-Notes 章节 1 初识机器学习课时1 欢迎参加《机器学习》课程第一个视频主要是讲什么是机器学习，介绍了一下机器学习能干什么（可以跳过） 课时2 什么是机器学习机器学习是什么？ 来自卡内基梅隆大学Tom Mitchel定义的机器学习为：一个程序被认为能从经验E中学习，解"><meta name="keywords" content="吴恩达机器学习"><meta name="author" content="A"><meta name="copyright" content="A"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://arthurblog.oss-cn-beijing.aliyuncs.com/banana.png"><link rel="canonical" href="https://arthurdream.gitee.io/2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="机器学习笔记（一）"><meta property="og:url" content="https://arthurdream.gitee.io/2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><meta property="og:site_name" content="AのBlog"><meta property="og:description" content="学习资源网易：吴恩达《机器学习》 B站：[中英字幕]吴恩达机器学习系列课程 其他资料：Coursera-ML-AndrewNg-Notes 章节 1 初识机器学习课时1 欢迎参加《机器学习》课程第一个视频主要是讲什么是机器学习，介绍了一下机器学习能干什么（可以跳过） 课时2 什么是机器学习机器学习是什么？ 来自卡内基梅隆大学Tom Mitchel定义的机器学习为：一个程序被认为能从经验E中学习，解"><meta property="og:image" content="https://arthurblog.oss-cn-beijing.aliyuncs.com/中川梨花2.jpeg"><meta property="article:published_time" content="2020-10-23T00:00:27.000Z"><meta property="article:modified_time" content="2020-10-25T13:07:40.091Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'true'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/ArthurDream/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="Docker基础" href="https://arthurdream.gitee.io/2020/10/29/Docker/"><link rel="next" title="Redis心得（转载）" href="https://arthurdream.gitee.io/2020/10/22/Redis%E5%BF%83%E5%BE%97%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/ArthurDream/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://arthurblog.oss-cn-beijing.aliyuncs.com/头像.jpg" onerror="onerror=null;src='https://arthurblog.oss-cn-beijing.aliyuncs.com/01cea05755a4d06ac72525ae9cadcd.jpg@900w_1l_2o_100sh.jpg'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/ArthurDream/archives/"><div class="headline">文章</div><div class="length_num">17</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/ArthurDream/tags/"><div class="headline">标签</div><div class="length_num">13</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/ArthurDream/categories/"><div class="headline">分类</div><div class="length_num">9</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ArthurDream/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/bangumis/"><i class="fa-fw fas fa-video"></i><span> 番剧</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#学习资源"><span class="toc-number">1.</span> <span class="toc-text">学习资源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节-1-初识机器学习"><span class="toc-number">2.</span> <span class="toc-text">章节 1 初识机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#课时1-欢迎参加《机器学习》课程"><span class="toc-number">2.1.</span> <span class="toc-text">课时1 欢迎参加《机器学习》课程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时2-什么是机器学习"><span class="toc-number">2.2.</span> <span class="toc-text">课时2 什么是机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时3-监督学习"><span class="toc-number">2.3.</span> <span class="toc-text">课时3 监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时4-无监督学习"><span class="toc-number">2.4.</span> <span class="toc-text">课时4 无监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时5-一些问题"><span class="toc-number">2.5.</span> <span class="toc-text">课时5 一些问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节-2-单变量线性回归"><span class="toc-number">3.</span> <span class="toc-text">章节 2 单变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#课时6-模型描述"><span class="toc-number">3.1.</span> <span class="toc-text">课时6 模型描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时7-代价函数"><span class="toc-number">3.2.</span> <span class="toc-text">课时7 代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时8-代价函数的直观理解（一）"><span class="toc-number">3.3.</span> <span class="toc-text">课时8 代价函数的直观理解（一）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时9-代价函数的直观理解（二）"><span class="toc-number">3.4.</span> <span class="toc-text">课时9 代价函数的直观理解（二）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时10-梯度下降"><span class="toc-number">3.5.</span> <span class="toc-text">课时10 梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时11-梯度下降知识点总结（直观感受）"><span class="toc-number">3.6.</span> <span class="toc-text">课时11 梯度下降知识点总结（直观感受）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时13-课程总结（PPT可跳过）"><span class="toc-number">3.7.</span> <span class="toc-text">课时13 课程总结（PPT可跳过）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节3-线性代数回顾"><span class="toc-number">4.</span> <span class="toc-text">章节3 线性代数回顾</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#课时14至19"><span class="toc-number">4.1.</span> <span class="toc-text">课时14至19</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节4-软件安装配置"><span class="toc-number">5.</span> <span class="toc-text">章节4 软件安装配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#课时20至26"><span class="toc-number">5.1.</span> <span class="toc-text">课时20至26</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节5-多变量线性回归"><span class="toc-number">6.</span> <span class="toc-text">章节5 多变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#课时27多维特征"><span class="toc-number">6.1.</span> <span class="toc-text">课时27多维特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时28-多变量梯度下降"><span class="toc-number">6.2.</span> <span class="toc-text">课时28-多变量梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时29-梯度下降法实践1-特征缩放"><span class="toc-number">6.3.</span> <span class="toc-text">课时29 梯度下降法实践1-特征缩放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时30-梯度下降法实践2-学习率"><span class="toc-number">6.4.</span> <span class="toc-text">课时30 梯度下降法实践2-学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时31-特征和多项式回归"><span class="toc-number">6.5.</span> <span class="toc-text">课时31 特征和多项式回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时32-正规方程"><span class="toc-number">6.6.</span> <span class="toc-text">课时32 正规方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#课时33-——35不重要，略过"><span class="toc-number">6.7.</span> <span class="toc-text">课时33 ——35不重要，略过</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节6-Octave-Matlab教程"><span class="toc-number">7.</span> <span class="toc-text">章节6 Octave&#x2F;Matlab教程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#章节7-Logistic回归"><span class="toc-number">8.</span> <span class="toc-text">章节7 Logistic回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#课时44——分类"><span class="toc-number">8.1.</span> <span class="toc-text">课时44——分类</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><header class="post-bg" id="page-header" style="background-image: url(https://arthurblog.oss-cn-beijing.aliyuncs.com/中川梨花2.jpeg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/ArthurDream/">AのBlog</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/ArthurDream/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/bangumis/"><i class="fa-fw fas fa-video"></i><span> 番剧</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div><div class="menus_item"><a class="site-page" href="/ArthurDream/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">机器学习笔记（一）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-10-23 08:00:27"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-10-23</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-10-25 21:07:40"><i class="fas fa-history fa-fw"></i> 更新于 2020-10-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/ArthurDream/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">3.6k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 12 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="far fa-comments fa-fw post-meta__icon"></i><span>评论数:</span><a href="/ArthurDream/2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/ArthurDream/2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h2><p>网易：<a href="https://study.163.com/course/courseMain.htm?courseId=1210076550&amp;_trace_c_p_k2_=0a2e4dc11a114923ae0162cda9d5b46a" target="_blank" rel="noopener">吴恩达《机器学习》</a></p>
<p>B站：<a href="https://www.bilibili.com/video/BV164411b7dx?from=search&amp;seid=13232445094666472951" target="_blank" rel="noopener">[中英字幕]吴恩达机器学习系列课程</a></p>
<p>其他资料：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">Coursera-ML-AndrewNg-Notes</a></p>
<h2 id="章节-1-初识机器学习"><a href="#章节-1-初识机器学习" class="headerlink" title="章节 1 初识机器学习"></a>章节 1 初识机器学习</h2><h3 id="课时1-欢迎参加《机器学习》课程"><a href="#课时1-欢迎参加《机器学习》课程" class="headerlink" title="课时1 欢迎参加《机器学习》课程"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280908399&amp;courseId=1210076550" target="_blank" rel="noopener">课时1 欢迎参加《机器学习》课程</a></h3><p>第一个视频主要是讲什么是机器学习，介绍了一下机器学习能干什么（可以跳过）</p>
<h3 id="课时2-什么是机器学习"><a href="#课时2-什么是机器学习" class="headerlink" title="课时2 什么是机器学习"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280907405&amp;courseId=1210076550" target="_blank" rel="noopener">课时2 什么是机器学习</a></h3><p><strong>机器学习是什么？</strong></p>
<p>来自卡内基梅隆大学Tom Mitchel定义的机器学习为：一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当有了经验E后，经过P评判，程序在处理T时的性能有所提升。</p>
<p>简单来说：<strong>机器学习是用机器学习算法来建立模型，当有新的数据过来时，可以通过模型来进行预测。</strong></p>
<p>为方便理解，把机器学习的过程与人类对历史经验归纳的过程做个对比如下：</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/什么是机器学习.png" alt="什么是机器学习"></p>
<h3 id="课时3-监督学习"><a href="#课时3-监督学习" class="headerlink" title="课时3 监督学习"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280910348&amp;courseId=1210076550" target="_blank" rel="noopener">课时3 监督学习</a></h3><p><strong>监督学习：</strong> 基本思想是，我们数据集中的每个样本都有相应的”正确答案“。再根据这些样本做出预测，就像房子问题和肿瘤问题</p>
<p><strong>（1）回归问题：</strong>通过回归来推出一个连续的输出</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/监督学习与回归问题.png" alt="监督学习与回归问题"></p>
<p><strong>（2）分类问题</strong>：分类指的是我们尝试推测出离散的输出值：0或1，良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2。0代表良性，1表示1类乳腺癌，2表示第2类乳腺癌，3表示第三类，这种也是分类问题。</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/分类问题.png" alt="分类问题"></p>
<p><strong>小测验</strong>：假设你经营着一家公司，你想开发学习算法来处理这两个问题</p>
<p>a.你有一大批同样的货物，想象一下，你有上千件一模一样的货物等待出售，这时你</p>
<p>想预测接下来的三个月能卖多少件？这是个回归问题</p>
<p>b.你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，</p>
<p>你要判断它们是否曾经被盗过？这是个分类问题</p>
<h3 id="课时4-无监督学习"><a href="#课时4-无监督学习" class="headerlink" title="课时4 无监督学习"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280912351&amp;courseId=1210076550" target="_blank" rel="noopener">课时4 无监督学习</a></h3><p><strong>无监督学习</strong>：它是学习策略，交给算法大量的数据，并让算法为我们从数据中找出某种结构。无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。针对数据集，无监督学习就能判断出数据有两个不同的聚集簇，所以叫做<strong>聚类算法</strong>。”这是一个，那是另一个，二者不同“。</p>
<p>这里把监督学习与无监督学习做一个直观的对比。</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/监督学习与无监督学习.png" alt="监督学习与无监督学习"></p>
<p>可以看到<strong>监督学习</strong>的每个数据集中每条数据都已经表明了是阴性或阳性，即是良性或恶性肿瘤。所以，对于监督学习里的每条数据，我们已经清楚地知道，训练集对应的正确答案，是良性或恶性了。</p>
<p>在<strong>无监督学习</strong>中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。无监督学习算法可能会把这些数据分成两个不同的簇，所以叫做聚类算法。</p>
<p><strong>举个例子</strong>：</p>
<p>垃圾邮件问题，如果你有标记好的数据，区别是垃圾还是非垃圾邮件，这是监督学习问题。</p>
<p>新闻事件分类：谷歌新闻将许多新闻分组，组成有关联的新闻，它做的就是搜索非常多的新闻事件，自动把他们聚类在一起。这是无监督学习问题。</p>
<h3 id="课时5-一些问题"><a href="#课时5-一些问题" class="headerlink" title="课时5 一些问题"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/text?lessonId=1280914356&amp;courseId=1210076550" target="_blank" rel="noopener">课时5 一些问题</a></h3><h2 id="章节-2-单变量线性回归"><a href="#章节-2-单变量线性回归" class="headerlink" title="章节 2 单变量线性回归"></a>章节 2 单变量线性回归</h2><p><strong>在进行章节2之前，先在此处说明一下描述这个回归问题的标记：</strong></p>
<p>𝑚 代表训练集中实例的数量</p>
<p>𝑥 代表特征/输入变量</p>
<p>𝑦 代表目标变量/输出变量</p>
<p>(𝑥, 𝑦) 代表训练集中的实例 </p>
<p>(𝑥(𝑖), 𝑦(𝑖)) 代表第𝑖 个观察实例</p>
<p>ℎ 代表学习算法的解决方案或函数也称为假设（hypothesis）</p>
<h3 id="课时6-模型描述"><a href="#课时6-模型描述" class="headerlink" title="课时6 模型描述"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280913357&amp;courseId=1210076550" target="_blank" rel="noopener">课时6 模型描述</a></h3><p><strong>公式：</strong>$h(x)=\theta_{0}+\theta_{1}x$， 这里的$\theta_{0}$和$\theta_{1}$代表参数。</p>
<p>因为只含有一个特征/输入变量，因此这样的问题叫作<strong>单变量线性回归问题</strong>。</p>
<h3 id="课时7-代价函数"><a href="#课时7-代价函数" class="headerlink" title="课时7 代价函数"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280912352&amp;courseId=1210076550" target="_blank" rel="noopener">课时7 代价函数</a></h3><p><strong>公式：</strong>$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$</p>
<p><strong>建模误差：</strong>我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是<strong>建模误差</strong>（<strong>modeling error</strong>）。</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/建模误差.png" alt="建模误差"></p>
<p>我们的目标便是<strong>选择出</strong>可以使得建模误差的平方和能够最小的<strong>模型参数</strong>，即使得代价函数$J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$最小。</p>
<p>代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是<strong>回归问题，都是一个合理的选择</strong>。</p>
<h3 id="课时8-代价函数的直观理解（一）"><a href="#课时8-代价函数的直观理解（一）" class="headerlink" title="课时8 代价函数的直观理解（一）"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280909359&amp;courseId=1210076550" target="_blank" rel="noopener">课时8 代价函数的直观理解（一）</a></h3><p>这里假设$\theta_0$=0，然后<img src= "/ArthurDream/img/loading.gif" data-src="https://math.jianshu.com/math?formula=%5Ctheta_1" alt="\theta_1">取不同的值画出函数</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/代价函数的直观理解1.png" alt="代价函数直观理解1"></p>
<h3 id="课时9-代价函数的直观理解（二）"><a href="#课时9-代价函数的直观理解（二）" class="headerlink" title="课时9 代价函数的直观理解（二）"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280909359&amp;courseId=1210076550" target="_blank" rel="noopener">课时9 代价函数的直观理解（二）</a></h3><p>代价函数的样子，等高线图，则可以看出在三维空间中存在一个使得𝐽(𝜃0, 𝜃1)最小的点</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/代价函数三维.png" alt="代价函数三维"></p>
<h3 id="课时10-梯度下降"><a href="#课时10-梯度下降" class="headerlink" title="课时10 梯度下降"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280907406&amp;courseId=1210076550" target="_blank" rel="noopener">课时10 梯度下降</a></h3><p><strong>梯度下降算法</strong>是一个用来求代价函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0,\theta_1)$的最小值。<strong>选择不同的初始参数组合，可能会找到不同的局部最小值。</strong></p>
<p>（背后的思想是：开始时我们随机选择一个参数的组合(𝜃0, 𝜃1, . . . . . . , 𝜃𝑛)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。）</p>
<p><strong>公式：</strong></p>
<p><strong>$Gradient\ descent\ algorithm$:</strong></p>
<p>$repeat$  $until$  $convergence$ {</p>
<p>​        $\theta_{j} := \theta_{j} - \alpha \frac{\partial }{\partial \theta_{j}}J(\theta_{0},\theta_{1})$    ( for j=0 and j=1 )</p>
<p>}</p>
<p>$Correct: Simultaneous\ update$</p>
<p> $temp0:=\theta_0- \alpha \frac{\partial }{\partial \theta_{0}}J(\theta_{0},\theta_{1})$</p>
<p>$temp1:=\theta_{1} - \alpha \frac{\partial }{\partial \theta_{1}}J(\theta_{0},\theta_{1})$</p>
<p>$\theta_0\ :=temp0$</p>
<p>$\theta_1\ :=temp1$</p>
<p>公式符号解析：</p>
<p>$:=$ ，赋值</p>
<p>$\alpha$  :learning rate，学习率。</p>
<p>$\frac{\partial }{\partial \theta_{j}}J(\theta_{0},\theta_{1})$,这是个微分项，以后会有详细解释。</p>
<p><strong>Correct: Simultaneous update</strong>的意思是同时把$\theta_{0},\theta_{1}$代入公式后计算出的值再赋值给$\theta_{0},\theta_{1}$,<strong>也就是用一对参数同时经过公式计算后的结果再更新参数，即同步更新参数</strong>。<u><strong>梯度下降要同时更新</strong></u><strong>!</strong></p>
<h3 id="课时11-梯度下降知识点总结（直观感受）"><a href="#课时11-梯度下降知识点总结（直观感受）" class="headerlink" title="课时11 梯度下降知识点总结（直观感受）"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280910349&amp;courseId=1210076550" target="_blank" rel="noopener">课时11 梯度下降知识点总结（直观感受）</a></h3><p><strong>$Gradient\ descent\ algorithm$:</strong></p>
<p>$\theta_{j} := \theta_{j} - \alpha \frac{\partial }{\partial \theta_{j}}J(\theta_{0},\theta_{1})$  </p>
<p>$Description$：对𝜃赋值，使得𝐽(𝜃)按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中𝑎是学习率（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/梯度下降算法的直观感受.png" alt="梯度下降算法的直观感受"></p>
<p>求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，这个导数是切率，此处这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的𝜃1，𝜃1更新后等于𝜃1减去一个正数乘以𝑎。</p>
<p><strong>来看一个例子：</strong></p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/梯度下降算法例子.png" alt="梯度下降算法例子"></p>
<p>意思为当我们逐渐接近局部最小值，梯度下降算法的步子将自动的越迈越小，因为局部最低点的导数是等于0，我们接近最低点时，导数值会愈来愈恶小，梯度下降会自动采取较小的幅度，所以不需要另外减少$\alpha$。</p>
<p>可参考简书<strong>云上听风</strong>理解为（底部附有原文链接）：</p>
<p>我们的目的是让代价函数$J(\theta)$最小，而代价函数的大小是由多个参数$\theta$决定的，所以我们是要求出合适的)$\theta$参数。 一开始先随机取$\theta$，然后用梯度下降不断更新$\theta$，学习率就是更新的步伐，每次移动一点，当找到局部最优点时导数为0，此时$\theta$停止更新(减零等于没有更新)。所以梯度下降可以让我们找到局部最优点(局部最低点)。</p>
<h3 id="课时13-课程总结（PPT可跳过）"><a href="#课时13-课程总结（PPT可跳过）" class="headerlink" title="课时13 课程总结（PPT可跳过）"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/text?lessonId=1280915338&amp;courseId=1210076550" target="_blank" rel="noopener">课时13 课程总结（PPT可跳过）</a></h3><h2 id="章节3-线性代数回顾"><a href="#章节3-线性代数回顾" class="headerlink" title="章节3 线性代数回顾"></a><strong>章节3 线性代数回顾</strong></h2><h3 id="课时14至19"><a href="#课时14至19" class="headerlink" title="课时14至19"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280913358&amp;courseId=1210076550" target="_blank" rel="noopener">课时14至19</a></h3><p>均是非常基础的线性代数知识，此处略过</p>
<h2 id="章节4-软件安装配置"><a href="#章节4-软件安装配置" class="headerlink" title="章节4 软件安装配置"></a><strong>章节4 软件安装配置</strong></h2><h3 id="课时20至26"><a href="#课时20至26" class="headerlink" title="课时20至26"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/text?lessonId=1280908414&amp;courseId=1210076550" target="_blank" rel="noopener">课时20至26</a></h3><p>都是PPT，没啥好说的，直接略过</p>
<h2 id="章节5-多变量线性回归"><a href="#章节5-多变量线性回归" class="headerlink" title="章节5 多变量线性回归"></a><strong>章节5 多变量线性回归</strong></h2><h3 id="课时27多维特征"><a href="#课时27多维特征" class="headerlink" title="课时27多维特征"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280912355&amp;courseId=1210076550" target="_blank" rel="noopener">课时27多维特征</a></h3><p>现在我们对房价模型增加更多的特征，</p>
<p>例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为(𝑥1, 𝑥1, . . . , 𝑥𝑛)。</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/多维特征.png" alt="多维特征"></p>
<p><strong>引入新的注释：</strong></p>
<p>$n$代表特征的数量。</p>
<p>𝑥(𝑖)代表第 𝑖 个训练实例，是特征矩阵中的第𝑖行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>上面的$x_0$值固定为1，是我们为了下面的矩阵运算而添加上去的。</p>
<p><strong>公式：</strong>$h_{\theta}(x)=\theta^TX=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$</p>
<p>$x_{j}^{(i)}$代表特征矩阵中第$i$行的第$j$个特征，也就是第<img src= "/ArthurDream/img/loading.gif" data-src="https://math.jianshu.com/math?formula=i" alt="i">个训练实例的第$j$个特征。 </p>
<p>$\theta^TX$中的$\theta$是列向量$\begin{bmatrix} \theta_0\\ \theta_1\\ …\\\theta_n\end{bmatrix}$,$X$为$\begin{bmatrix} x_0\\ x_1\\ …\\x_n\end{bmatrix}$</p>
<p>$\theta$转置后为：$\begin{bmatrix}\theta_0 &amp;\theta_1 &amp;… &amp;\theta_n \end{bmatrix}$，然后与列向量$X$相乘，自然就是$\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$。</p>
<h3 id="课时28-多变量梯度下降"><a href="#课时28-多变量梯度下降" class="headerlink" title="课时28-多变量梯度下降"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280913359&amp;courseId=1210076550" target="_blank" rel="noopener">课时28-多变量梯度下降</a></h3><p>构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：</p>
<p>$J(\theta_0,\theta_1,…,\theta_n,)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$,</p>
<p>其中，$h_{\theta}(x)=\theta^TX=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$</p>
<p>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。多变量线性回归的批量梯度下降算法为：</p>
<p>（懒得打了，直接截图吧）</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/多变量梯度下降算法 .png" alt="多变量梯度下降算法"></p>
<p>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛</p>
<p>​        代码示例：</p>
<p>​        计算代价函数$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$, 其中：$h_{\theta}(x)=\theta^TX=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$ </p>
<p><strong>Python</strong> 代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br></pre></td></tr></table></figure>
<h3 id="课时29-梯度下降法实践1-特征缩放"><a href="#课时29-梯度下降法实践1-特征缩放" class="headerlink" title="课时29 梯度下降法实践1-特征缩放"></a><strong><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280909361&amp;courseId=1210076550" target="_blank" rel="noopener">课时29 梯度下降法实践1-特征缩放</a></strong></h3><p><strong>特征缩放</strong>可以使梯度下降<strong>运行的更快</strong>。</p>
<p>最简单的方法是令：$x_n=\frac{x_n-\mu_n}{s_n}$,其中$\mu_n$是平均值，$s_n$是标准差（标准差是方差的算术平方根，标准差能反映一个数据集的离散程度）</p>
<h3 id="课时30-梯度下降法实践2-学习率"><a href="#课时30-梯度下降法实践2-学习率" class="headerlink" title="课时30 梯度下降法实践2-学习率"></a><strong><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280909361&amp;courseId=1210076550" target="_blank" rel="noopener">课时30 梯度下降法实践2-学习率</a></strong></h3><p>学习率太小可能运行的太慢，学习率太大时可能不会在每次迭代时都下降，甚至可能不收敛。</p>
<p>通常，学习率<img src= "/ArthurDream/img/loading.gif" data-src="https://math.jianshu.com/math?formula=%5Calpha" alt="\alpha">的取值可以为：</p>
<p>𝛼 = 0.01<em>，</em>0.03<em>，</em>0.1<em>，</em>0.3<em>，</em>1<em>，</em>3<em>，</em>10</p>
<h3 id="课时31-特征和多项式回归"><a href="#课时31-特征和多项式回归" class="headerlink" title="课时31 特征和多项式回归"></a><strong><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280909361&amp;courseId=1210076550" target="_blank" rel="noopener">课时31 特征和多项式回归</a></strong></h3><p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据。</p>
<p>比如一个:二次方模型：$h_\theta=\theta_0+\theta_1x_1+\theta_2 x_2^2$  ，或一个三次方模型：$h_\theta=\theta_0+\theta_1x_1+\theta_2 x_2^2+\theta_3 x_3^3$</p>
<p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 我们可以令：$x_2= x_2^2$，$x_3= x_3^3$，从而将模型转化为线性回归模型</p>
<h3 id="课时32-正规方程"><a href="#课时32-正规方程" class="headerlink" title="课时32 正规方程"></a><strong><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280909361&amp;courseId=1210076550" target="_blank" rel="noopener">课时32 正规方程</a></strong></h3><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数：$ \frac{\partial }{\partial \theta_{j}}J(\theta_j)=0$。 </p>
<p>假设我们的训练集特征矩阵为 𝑋（包含了 $x_0$= 1）并且我们的训练集结果为向量 𝑦，则利用正规方程解出向量$\theta=(X^{T}X)^{-1}X^{T}y$</p>
<p>上标 <strong>T</strong> 代表矩阵转置，上标-1 代表矩阵的逆。设矩阵𝐴 = $X^{T}X$，则：$(X^{T}X)^{-1}$ =$A^{-1}$</p>
<p>以下表数据为例：</p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/正规方程.png" alt="正规方程"></p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/正规方程求解参数.png" alt="正规方程方法求解参数"></p>
<p><img src= "/ArthurDream/img/loading.gif" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/梯度下降和正规方程对比.png" alt="梯度下降和正规方程对比"></p>
<p>正规方程的 <strong>python</strong> 实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(X, y)</span>:</span></span><br><span class="line"> 	theta = np.linalg.inv(X.T@X)@X.T@y <span class="comment">#X.T@X 等价于 X.T.dot(X)</span></span><br><span class="line"> 	<span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<h3 id="课时33-——35不重要，略过"><a href="#课时33-——35不重要，略过" class="headerlink" title="课时33 ——35不重要，略过"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280911413&amp;courseId=1210076550" target="_blank" rel="noopener">课时33 ——35不重要，略过</a></h3><h2 id="章节6-Octave-Matlab教程"><a href="#章节6-Octave-Matlab教程" class="headerlink" title="章节6 Octave/Matlab教程"></a>章节6 Octave/Matlab教程</h2><p>没啥好说的，略过</p>
<h2 id="章节7-Logistic回归"><a href="#章节7-Logistic回归" class="headerlink" title="章节7 Logistic回归"></a>章节7 Logistic回归</h2><p><strong>引言：</strong></p>
<p>逻辑回归为什么叫逻辑回归？是因为使用了sigmoid函数，sigmoid函数本身也是Logistic function的一种形式，故此得名。逻辑回归是一种广义的线性回归分析模型，Sigmoid函数是逻辑回归的核心，通过Sigmoid函数将原本的线性回归问题转化成了一个分类问题。 Logistic回归是用来解决二类分类问题的，如果要解决的问题是多分类问题呢？那就要用到softmax回归了，它是Logistic回归在多分类问题上的推广。此处神经网络模型开始乱入，softmax回归一般用于神经网络的输出层，此时输出层叫做softmax层。(摘自<a href="https://www.jianshu.com/p/8cfb036136f5" target="_blank" rel="noopener">云上听风</a>)</p>
<h3 id="课时44——分类"><a href="#课时44——分类" class="headerlink" title="课时44——分类"></a><a href="https://study.163.com/course/courseLearn.htm?courseId=1210076550#/learn/video?lessonId=1280911413&amp;courseId=1210076550" target="_blank" rel="noopener">课时44——分类</a></h3><p>今天先写到这里</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">A</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://arthurdream.gitee.io/2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/">https://arthurdream.gitee.io/2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://arthurdream.gitee.io" target="_blank">AのBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/ArthurDream/tags/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">吴恩达机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://arthurblog.oss-cn-beijing.aliyuncs.com/gakii.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/ArthurDream/2020/10/29/Docker/"><img class="prev-cover" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/gakii.png" onerror="onerror=null;src='https://arthurblog.oss-cn-beijing.aliyuncs.com/01cea05755a4d06ac72525ae9cadcd.jpg@900w_1l_2o_100sh.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Docker基础</div></div></a></div><div class="next-post pull-right"><a href="/ArthurDream/2020/10/22/Redis%E5%BF%83%E5%BE%97%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"><img class="next-cover" data-src="https://arthurblog.oss-cn-beijing.aliyuncs.com/中川梨花1.png" onerror="onerror=null;src='https://arthurblog.oss-cn-beijing.aliyuncs.com/01cea05755a4d06ac72525ae9cadcd.jpg@900w_1l_2o_100sh.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Redis心得（转载）</div></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var requestSetting = function (from,set) {
  var from = from
  var setting = set.split(',').filter(function(item){
  return from.indexOf(item) > -1
  });
  setting = setting.length == 0 ? from :setting;
  return setting
}

var guestInfo = requestSetting(['nick','mail','link'],'nick,mail,link')
var requiredFields = requestSetting(['nick','mail'],'nick,mail')

window.valine = new Valine({
  el:'#vcomment',
  appId: '632tS4XoORh0cyf2mrWWH02U-gzGzoHsz',
  appKey: 'My5tHCUDUe9hOVKodK8NhSpq',
  placeholder: '记得留下你的昵称（或QQ）和邮箱....稍后回复你',
  avatar: 'mm',
  meta: guestInfo,
  pageSize: '10',
  lang: 'zh-CN',
  recordIP: true,
  serverURLs: '',
  emojiCDN: '//i0.hdslb.com/bfs/emote/',
  emojiMaps: {"2020":"dc709fac0d361370bcf0d36d32adb97df7c95824.png","热词系列-知识增加":"142409b595982b8210b2958f3d340f3b47942645.png","热词系列-好家伙":"63ec80dea3066bd9f449ba999ba531fa61f7b4eb.png","热词系列-芜湖起飞":"78d04c6ce78a613c90d510cd45fe7e25c57ba00b.png","热词系列-爷青回":"a26189ff1e681bddef7f6533f9aabe7604731a3e.png","热词系列-梦幻联动":"4809416be5ca787c2ec3e897e4fd022a58da6e0e.png","热词系列-泪目":"bba3703ab90b7d16fe9dbcb85ed949db687f8331.png","热词系列-保护":"55f8f6445ca7c3170cdfc5b16036abf639ce9b57.png","热词系列-害怕":"d77e2de26da143249f0c0ad7a608c27152c985bf.png","热词系列-爱了爱了":"2a165b555ba20391316366c664ed7891883dc5aa.png","热词系列-吹爆":"b528220f9c37256ed6a37f05bf118e44b08b81e5.png","热词系列-三连":"21f15fe11b7a84d2f2121c16dec50a4e4556f865.png","热词系列-可以":"e08543c71202b36c590094417fcfbb80c3506cd8.png","热词系列-希望没事":"6c0d2e6c486d1ba5afd6204a96e102652464a01d.png","热词系列-打卡":"a9cf77c78e1b9b40aa3ed4862402fba008ee2f51.png@","热词系列-skr":"bd285ff94db16ad52557c3effe930d64663e8375.png","热词系列-battle":"f2f81c8e47db6252becd633a5d1ee14e15df2ea8.png","热词系列-DNA":"f6eb74f8230588f61a298af89061a7d75c5762e5.png","热词系列-妙啊":"0e98299d7decf5eaffad854977946075c3e91cb8.png","热词系列-这次一定":"a01ca28923daa7cc896c42f27deb4914e20dd572.png","热词系列-AWSL":"c37f88cf799f9badf9d84b7671dc3dd98c0fc0c2.png","热词系列-递话筒":"98e6950e39fbb4dd1c576042063ca632074070ba.png","热词系列-你细品":"535e00658e7e47966f154d3a167fa2365ebc4321.png","热词系列-咕咕":"d8065c2e7ce48c929317a94553499a46fecc262a.png","热词系列-标准结局":"3de98174b510cf7dc5fd1bd08c5d881065e79137.png","热词系列-危":"5cc6c3357c4df544dd8de9d5c5c0cec97c7c9a56.png","热词系列-张三":"255a938f39cea625032b6650036b31aa26c50a3c.png","热词系列-害":"cbe798a194612958537c5282fcca7c3bcd2aa15c.png","热词系列-我裂开了":"29bd57ec4e8952880fea6c9e47aee924e91f10c4.png","热词系列-有内味了":"7ca61680a905b5b6e2e335c630e725b648b03b4d.png","热词系列-猛男必看":"c97064450528a0e45c7e7c365a15fbb13fd61d8c.png","热词系列-奥力给":"c9b8683827ec6c00fea5327c9bec14f581cef2aa.png","热词系列-问号":"c1d1e76c12180adc8558f47006fe0e7ded4154bb.png","热词系列-我哭了":"9e0b3877d649aaf6538fbdd3f937e240a9d808e4.png","热词系列-高产":"9db817cba4a7f4a42398f3b2ec7c0a8e0c247c42.png","热词系列-我酸了":"a8cbf3f6b8cd9377eeb15b9172f3cd683b2e4650.png","热词系列-真香":"e68497c775feaac1c3b1a6cd63a50cfb11b767c4.png","热词系列-我全都要":"d424d1ad8d14c1c9b8367842bc68c658b9229bc1.png","热词系列-神仙UP":"a49e0d0db1e7d35a0f7411be13208951ab448f03.png","热词系列-你币有了":"84820c2b147a8ca02f3c4006b63f76c6313cbfa0.png","热词系列-不愧是你":"9ff2e356797c57ee3b1675ade0883d2d2247be9b.png","热词系列-锤":"35668cc12ae25b9545420e4a85bf21a0bfc03e5d.png","热词系列-秀":"50782fbf5d9b7f48f9467b5c53932981e321eedc.png","热词系列-爷关更":"faad40c56447f1f8abcb4045c17ce159d113d1fd.png","热词系列-有生之年":"f41fdafe2d0fbb8e8bc1598d2cf37e355560103a.png","热词系列-镇站之宝":"24e7a6a6e6383c987215fb905e3ee070aca259b5.png","热词系列-我太南了":"a523f3e4c63e4db1232365765d0ec452f83be97e.png","热词系列-完结撒花":"ea9db62ff5bca8e069cd70c4233353a802835422.png","热词系列-大师球":"f30089248dd137c568edabcb07cf67e0f6e98cf3.png","热词系列-知识盲区":"ccc94600b321a28116081e49ecedaa4ee8728312.png","热词系列-“狼火”":"33ccd3617bfa89e9d1498b13b7542b63f163e5de.png","热词系列-你可真星":"54c8ddff400abfe388060cabfbb579280fdea1be.png","微笑":"685612eadc33f6bc233776c6241813385844f182.png","呲牙":"b5a5898491944a4268360f2e7a84623149672eb6.png","OK":"4683fd9ffc925fa6423110979d7dcac5eda297f4.png","星星眼":"63c9d1a31c0da745b61cdb35e0ecb28635675db2.png","哦呼":"362bded07ea5434886271d23fa25f5d85d8af06c.png","歪嘴":"4384050fbab0586259acdd170b510fe262f08a17.png","嫌弃":"de4c0783aaa60ec03de0a2b90858927bfad7154b.png","喜欢":"8a10a4d73a89f665feff3d46ca56e83dc68f9eb8.png","酸了":"92b1c8cbceea3ae0e8e32253ea414783e8ba7806.png","大哭":"2caafee2e5db4db72104650d87810cc2c123fc86.png","害羞":"9d2ec4e1fbd6cb1b4d12d2bbbdd124ccb83ddfda.png","疑惑":"b7840db4b1f9f4726b7cb23c0972720c1698d661.png","辣眼睛":"35d62c496d1e4ea9e091243fa812866f5fecc101.png","调皮":"8290b7308325e3179d2154327c85640af1528617.png","喜极而泣":"485a7e0c01c2d70707daae53bee4a9e2e31ef1ed.png","奸笑":"bb84906573472f0a84cebad1e9000eb6164a6f5a.png","笑":"81edf17314cea3b48674312b4364df44d5c01f17.png","偷笑":"6c49d226e76c42cd8002abc47b3112bc5a92f66a.png","大笑":"ca94ad1c7e6dac895eb5b33b7836b634c614d1c0.png","阴险":"ba8d5f8e7d136d59aab52c40fd3b8a43419eb03c.png","捂脸":"6921bb43f0c634870b92f4a8ad41dada94a5296d.png","囧":"12e41d357a9807cc80ef1e1ed258127fcc791424.png","呆":"33ad6000d9f9f168a0976bc60937786f239e5d8c.png","抠鼻":"cb89184c97e3f6d50acfd7961c313ce50360d70f.png","惊喜":"0afecaf3a3499479af946f29749e1a6c285b6f65.png","惊讶":"f8e9a59cad52ae1a19622805696a35f0a0d853f3.png","笑哭":"c3043ba94babf824dea03ce500d0e73763bf4f40.png","妙啊":"b4cb77159d58614a9b787b91b1cd22a81f383535.png","doge":"bba7c12aa51fed0199c241465560dfc2714c593e.png","滑稽":"d15121545a99ac46774f1f4465b895fe2d1411c3.png","吃瓜":"4191ce3c44c2b3df8fd97c33f85d3ab15f4f3c84.png","打call":"431432c43da3ee5aab5b0e4f8931953e649e9975.png","点赞":"1a67265993913f4c35d15a6028a30724e83e7d35.png","鼓掌":"895d1fc616b4b6c830cf96012880818c0e1de00d.png","无语":"44667b7d9349957e903b1b62cb91fb9b13720f04.png","尴尬":"cb321684ed5ce6eacdc2699092ab8fe7679e4fda.png","冷":"cb0ebbd0668640f07ebfc0e03f7a18a8cd00b4ed.png","灵魂出窍":"43d3db7d97343c01b47e22cfabeca84b4251f35a.png","委屈":"d2f26cbdd6c96960320af03f5514c5b524990840.png","傲娇":"010540d0f61220a0db4922e4a679a1d8eca94f4e.png","疼":"905fd9a99ec316e353b9bd4ecd49a5f0a301eabf.png","吓":"9c10c5ebc7bef27ec641b8a1877674e0c65fea5d.png","生病":"0f25ce04ae1d7baf98650986454c634f6612cb76.png","吐":"06946bfe71ac48a6078a0b662181bb5cad09decc.png","嘘声":"e64af664d20716e090f10411496998095f62f844.png","捂眼":"c5c6d6982e1e53e478daae554b239f2b227b172b.png","思考":"cfa9b7e89e4bfe04bbcd34ccb1b0df37f4fa905c.png","再见":"fc510306bae26c9aec7e287cdf201ded27b065b9.png","翻白眼":"eba54707c7168925b18f6f8b1f48d532fe08c2b1.png","哈欠":"888d877729cbec444ddbd1cf4c9af155a7a06086.png","奋斗":"bb2060c15dba7d3fd731c35079d1617f1afe3376.png","墨镜":"3a03aebfc06339d86a68c2d893303b46f4b85771.png","难过":"a651db36701610aa70a781fa98c07c9789b11543.png","撇嘴":"531863568e5668c5ac181d395508a0eeb1f0cda4.png","抓狂":"4c87afff88c22439c45b79e9d2035d21d5622eba.png","生气":"3195714219c4b582a4fb02033dd1519913d0246d.png","口罩":"3ad2f66b151496d2a5fb0a8ea75f32265d778dd3.png","月饼":"89b19c5730e08d6f12fadf6996de5bc2e52f81fe.png","视频卫星":"dce6fc7d6dfeafff01241924db60f8251cca5307.png","11周年":"d3b2d5dc028c75ae4df379f4c3afbe186d0f6f9b.png","鸡腿":"c7860392815d345fa69c4f00ef18d67dccfbd574.png","干杯":"8da12d5f55a2c7e9778dcc05b40571979fe208e6.png","爱心":"ed04066ea7124106d17ffcaf75600700e5442f5c.png","锦鲤":"643d6c19c8164ffd89e3e9cdf093cf5d773d979c.png","胜利":"b49fa9f4b1e7c3477918153b82c60b114d87347c.png","加油":"c7aaeacb21e107292d3bb053e5abde4a4459ed30.png","保佑":"fafe8d3de0dc139ebe995491d2dac458a865fb30.png","抱拳":"89516218158dbea18ab78e8873060bf95d33bbbe.png","响指":"1b5c53cf14336903e1d2ae3527ca380a1256a077.png","支持":"3c210366a5585706c09d4c686a9d942b39feeb50.png","拥抱":"41780a4254750cdaaccb20735730a36044e98ef3.png","怪我咯":"07cc6077f7f7d75b8d2c722dd9d9828a9fb9e46d.png","跪了":"f2b3aee7e521de7799d4e3aa379b01be032698ac.png","黑洞":"e90ec4c799010f25391179118ccd9f66b3b279ba.png","老鼠":"8e6fb491eb1bb0d5862e7ec8ccf9a3da12b6c155.png","福到了":"5de5373d354c373cf1617b6b836f3a8d53c5a655.png","高兴":"416570a8aca7be12fb2c36e4b846906653f6d294.png","气愤":"069b029d17a086ab475fd331697a649e234850bb.png","耍帅":"d7a38b08d1f1cc35b19c35041f29ffcc48808e87.png"},
  enableQQ: true,
  requiredFields: requiredFields
});</script></div></article></main><footer id="footer" style="background-image: url(https://arthurblog.oss-cn-beijing.aliyuncs.com/中川梨花2.jpeg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By A</div><div class="footer_custom_text">欢迎访问AのBlog</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">简</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fas fa-comments"></i></a><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/ArthurDream/js/utils.js"></script><script src="/ArthurDream/js/main.js"></script><script src="/ArthurDream/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/ArthurDream/js/third-party/fireworks.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/ArthurDream/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/ArthurDream/js/search/local-search.js"></script><script src="https://myhkw.cn/player/js/player.js" id="myhk" key="159421506086" m="1"></script><script>var isChatBtn = true

if (isChatBtn) {
  ((window.gitter = {}).chat = {}).options = {
    room: 'ArthurDream666/community',
    activationElement: '#chat_btn'
  };
} else {
  ((window.gitter = {}).chat = {}).options = {
    room: 'ArthurDream666/community',
  };

  if (false) {
    var gitterBtn = document.getElementsByClassName('gitter-open-chat-button')

    function chatBtnHide () {
      gitterBtn[0].style.display= 'none'
    }

    function chatBtnShow () {
      gitterBtn[0].style.display= 'block'
    }
  }
}
</script><script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async="async" defer="defer"></script><script src="/ArthurDream/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>